# J2KSwift vs OpenJPEG: Performance Comparison

> **Week 269–271 Deliverable** — Comprehensive performance analysis of J2KSwift
> against OpenJPEG (ISO/IEC 15444 reference implementation).

---

## Overview

This document describes the performance benchmarking methodology used to compare
J2KSwift against OpenJPEG, presents the performance targets defined in the v2.0
specification, and explains how to reproduce the benchmarks locally and in CI.

J2KSwift is designed to meet or exceed OpenJPEG performance on all supported
platforms, with particular emphasis on Apple Silicon where hardware acceleration
(ARM Neon, Accelerate framework, Metal GPU compute) provides significant advantages.

---

## Performance Targets (v2.0 Specification)

| Operation | Configuration | Apple Silicon Target | Intel x86-64 Target |
|-----------|--------------|---------------------|---------------------|
| Encode    | Lossless     | ≥ 1.5× faster       | ≥ 1.0× (parity)    |
| Encode    | Lossy        | ≥ 2.0× faster       | ≥ 1.2× faster      |
| Encode    | HTJ2K        | ≥ 3.0× faster       | ≥ 1.5× faster      |
| Decode    | All modes    | ≥ 1.5× faster       | ≥ 1.0× (parity)    |
| GPU Encode| Metal/Vulkan | ≥ 10× faster        | ≥ 5× faster (Vulkan)|

Speed ratio = OpenJPEG median time ÷ J2KSwift median time.
A ratio > 1.0 means J2KSwift is faster.

---

## Benchmark Framework

### Infrastructure

The benchmarking infrastructure is implemented in:

- **`Sources/J2KCore/J2KOpenJPEGBenchmark.swift`** — Core types:
  `BenchmarkImageSize`, `BenchmarkCodingMode`, `BenchmarkConfiguration`,
  `BenchmarkMetrics`, `OpenJPEGBenchmarkRunner`, `PerformanceRegressionDetector`,
  `BenchmarkReportGenerator`
- **`Tests/PerformanceTests/OpenJPEGBenchmark.swift`** — 80+ automated test cases
- **`Scripts/benchmark_openjpeg.sh`** — Shell script for local benchmarking

### Image Sizes

Six standardised image sizes are used across all configurations:

| Size       | Pixels      | MP      |
|------------|-------------|---------|
| 256 × 256  | 65,536      | 0.065   |
| 512 × 512  | 262,144     | 0.262   |
| 1024 × 1024| 1,048,576   | 1.048   |
| 2048 × 2048| 4,194,304   | 4.194   |
| 4096 × 4096| 16,777,216  | 16.777  |
| 8192 × 8192| 67,108,864  | 67.109  |

### Coding Modes

| Mode              | Standard     | Bit-rate  |
|-------------------|-------------|-----------|
| Lossless          | Part 1       | N/A       |
| Lossy 2 bpp       | Part 1       | 2.0 bpp   |
| Lossy 1 bpp       | Part 1       | 1.0 bpp   |
| Lossy 0.5 bpp     | Part 1       | 0.5 bpp   |
| HTJ2K Lossless    | Part 15      | N/A       |
| HTJ2K Lossy 2 bpp | Part 15      | 2.0 bpp   |

### Metrics Collected

For each benchmark run, the following metrics are recorded:

- **Wall-clock time** (min, median, average, max, std dev) — in milliseconds
- **Throughput** — megapixels per second (MP/s) and megabytes per second (MB/s)
- **Iterations** — number of measurement iterations (warm-up excluded)
- **Speed ratio** — OpenJPEG median ÷ J2KSwift median (requires OpenJPEG)

### Test Images

Synthetic test images are generated by `BenchmarkTestImageGenerator` using four
patterns:

| Pattern           | Description                              | Compressible? |
|-------------------|------------------------------------------|---------------|
| Random            | Independent uniformly random pixels      | No            |
| Gradient          | Smooth linear colour gradient            | Yes           |
| Checkerboard      | 16-pixel black/white checkerboard        | Yes           |
| Natural Photo     | Low-frequency smooth + 5-bit noise       | Moderate      |

All benchmarks use the **Natural Photo** pattern by default as it is most
representative of real-world photographic content.

---

## Benchmark Suites

Three pre-defined suites are provided via `BenchmarkConfiguration`:

| Suite                 | Sizes           | Modes          | Use Case                       |
|-----------------------|-----------------|----------------|-------------------------------|
| `ciSuite`             | 512, 1024       | Lossless, 2bpp | CI regression detection        |
| `singleThreadedSuite` | 512–2048        | Lossless, 2bpp | Apples-to-apples vs OpenJPEG  |
| `fullSuite`           | All (256–8192)  | All 6 modes    | Comprehensive performance audit|

---

## Running Benchmarks

### Swift Test (recommended for CI)

```bash
# CI suite (small images, fast)
swift test --filter PerformanceTests -c release

# All performance tests
swift test --filter OpenJPEGBenchmark -c release
```

### Shell Script (with OpenJPEG comparison)

```bash
# Install OpenJPEG (macOS)
brew install openjpeg

# Install OpenJPEG (Ubuntu/Debian)
apt-get install -y libopenjp2-tools

# Run benchmark script
chmod +x Scripts/benchmark_openjpeg.sh
./Scripts/benchmark_openjpeg.sh --sizes 512,1024,2048 --runs 5

# Custom output directory
./Scripts/benchmark_openjpeg.sh -o /tmp/my_benchmark_results -s 256,512,1024 -r 10

# J2KSwift only (no OpenJPEG required)
./Scripts/benchmark_openjpeg.sh --no-openjpeg
```

### Programmatic API

```swift
import J2KCore

// Run CI suite
let runner = OpenJPEGBenchmarkRunner(includeOpenJPEG: true)
let suite  = runner.run(configurations: BenchmarkConfiguration.ciSuite)

// Print text report
print(BenchmarkReportGenerator.textReport(suite))

// Save CSV for spreadsheet analysis
let csv = BenchmarkReportGenerator.csvReport(suite)
try csv.write(toFile: "results.csv", atomically: true, encoding: .utf8)

// Check for regressions against a stored baseline
let detector = PerformanceRegressionDetector(regressionThreshold: 0.05)
let regressions = detector.findRegressions(current: suite, baseline: loadBaseline())
if !regressions.isEmpty {
    print("⚠️  Performance regressions detected:")
    regressions.forEach { print($0.description) }
}
```

---

## CI Integration

The performance workflow (`.github/workflows/performance.yml`) runs:

1. **macOS (Apple Silicon)** — full `ciSuite` + OpenJPEG comparison (when available)
2. **Linux x86-64** — `ciSuite` + OpenJPEG comparison
3. **Linux ARM64** — `ciSuite` (Docker/QEMU)
4. **Regression detection** — compares against the stored baseline artifact

### Artifacts

Each CI run uploads:
- `benchmark-results-macos` — text + CSV results for macOS
- `benchmark-results-linux` — text + CSV results for Linux x86-64
- `benchmark-results-linux-arm64` — results for Linux ARM64
- `openjpeg-comparison` — side-by-side OpenJPEG comparison (when available)

Artifacts are retained for **30 days**.

### Regression Threshold

The regression detector is configured with a **5 % threshold**:
a run that is more than 5 % slower than the stored baseline triggers a CI warning.
The threshold can be tightened for specific configurations by adjusting
`PerformanceRegressionDetector(regressionThreshold:)`.

---

## Performance Analysis Methodology

### Single-threaded vs Multi-threaded

Benchmarks are run in two modes:

- **Single-threaded** (`singleThreadedSuite`) — both J2KSwift and OpenJPEG are
  invoked with a single thread to ensure an apples-to-apples comparison.
- **Multi-threaded** — J2KSwift uses its `TaskGroup`-based concurrent pipeline
  while OpenJPEG uses its built-in threading.

### Warm-up Iterations

Each benchmark includes warm-up iterations (default: 2) that are discarded before
measurement.  Warm-up ensures that:
- Swift runtime JIT compilation overhead is excluded
- CPU caches are in a representative state
- OpenJPEG CLI tool start-up cost is amortised

### Statistical Robustness

The **median** is used as the primary latency statistic, as it is more robust to
occasional outliers (e.g., OS scheduler interference) than the mean.  The standard
deviation is reported to quantify timing jitter.

---

## Identify and Address Performance Gaps

Where J2KSwift trails OpenJPEG, the following optimisation strategies are applied:

| Gap Area              | Strategy                                                    |
|-----------------------|-------------------------------------------------------------|
| DWT forward/inverse   | ARM Neon SIMD lifting (5/3 and 9/7), AVX2 on x86-64        |
| Entropy coding (MQ)   | Vectorised context formation, Neon/SSE bit-plane coder      |
| Colour transform      | Accelerate vDSP / Neon ICT/RCT, Metal GPU shader           |
| Quantisation          | Dead-zone scalar, SIMD batch, Metal GPU dispatch            |
| Tier-2 packet assembly| Cache-friendly packet headers, zero-copy buffer strategy    |
| HTJ2K                 | SIMD-optimised HT MQ-coder, parallel block processing       |

---

## Benchmark Result Archive

Historical benchmark results are stored as GitHub Actions artifacts.
To compare across builds:

1. Download the CSV artifacts from two runs.
2. Compute `speedRatio = baseline_medianMs / current_medianMs` per row.
3. Flag any row where `speedRatio < 0.95` (i.e., >5% slower).

A convenience script is available:

```bash
python3 Scripts/compare_performance.py \
    --baseline benchmark-results-baseline/benchmark-results.csv \
    --current  benchmark-results-current/benchmark-results.csv \
    --threshold 0.05
```

---

## Known Limitations and Trade-offs

| Limitation | Details |
|---|---|
| OpenJPEG comparison requires CLI tools | Tests skip gracefully when `opj_compress`/`opj_decompress` are absent. |
| GPU benchmarks need Apple Silicon or Vulkan driver | Covered in `J2KMetalTests` and `J2KVulkanTests` respectively. |
| 8192×8192 images excluded from CI suite | Too slow for per-PR CI; run locally with `fullSuite`. |
| HTJ2K OpenJPEG comparison requires OpenJPEG ≥ 2.5 | Older versions do not support HTJ2K. |
| Windows benchmarks not yet in CI | Pending Windows CI runner availability. |

---

## Version History

| Date       | Milestone | Notes |
|------------|-----------|-------|
| 2026-02-20 | Week 266–268 | OpenJPEG interoperability infrastructure complete. |
| 2026-02-20 | Week 269–271 | Performance benchmarking framework implemented. This document. |

---

*Last Updated*: 2026-02-20 (Week 269–271)
